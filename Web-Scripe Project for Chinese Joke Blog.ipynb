{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea02d38",
   "metadata": {},
   "source": [
    "# Web-Scripe Project for Chinese Joke Blog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279de5bb",
   "metadata": {},
   "source": [
    "## Part 1 Load requests and BeautifulSoup Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef80c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests                \n",
    "from bs4 import BeautifulSoup  \n",
    "import time                    \n",
    "import random                  \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73de25f",
   "metadata": {},
   "source": [
    "## Part 2 Define get_urls function which will acquire the each detail page link and get_info which will acquire target information from each detail page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed84c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(url):                   # Acquire urls of all detail page for each navigation page\n",
    "    urls = []\n",
    "    # Send Internet Request\n",
    "    response = requests.get(url=url)\n",
    "    # Create BeautifulSoup Object and parse the HTML\n",
    "    soup = BeautifulSoup(response.text, features=\"lxml\")\n",
    "    h2_all=soup.find_all(name='h2')      # Acquire h2 tage for each page \n",
    "    for h in h2_all:                     # For loop all h2 tag\n",
    "        # Acquire url of each h2 tage and append it into the list\n",
    "        urls.append(h.find('a')['href'])\n",
    "    return urls                           # Return urls of all detail page for each navigation page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2181094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(url):              # Acquire required information of each detail page\n",
    "    response = requests.get(url=url)\n",
    "    soup = BeautifulSoup(response.text, features=\"lxml\")\n",
    "    title = soup.find(class_='article-title').find('a').get_text() # Acquire title\n",
    "    spans = soup.find(class_='article-meta').find_all(name='span')\n",
    "    date = spans[0].get_text()       # Acquire Date\n",
    "    category = spans[1].get_text()       # Acquire Category\n",
    "    read = spans[2].get_text()       # Acquire Read\n",
    "    comment = spans[3].get_text()    # Acquire Comment\n",
    "    content = soup.find(class_='article-content').get_text() # Acquire Content\n",
    "    every_row=pd.DataFrame({\"Date\":[date],\"Category\":[category],\"Reading\":[read],\"Comments\":[comment],\"Content\":[content]})\n",
    "    return every_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd808bb",
   "metadata": {},
   "source": [
    "## Part 3 Extract Information and Generate Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3ee2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "navigative_urls=['https://duanzixing.com/page/{}/'.format(i) for i in range(1,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b7ca81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_columns=[]\n",
    "for i in range(len(navigative_urls)):\n",
    "    detail_urls=get_urls(navigative_urls[i])\n",
    "    for url in detail_urls:\n",
    "        individual_columns.append(get_info(url))\n",
    "        time.sleep(random.randint(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35042c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.concat(individual_columns).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56a4c009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date Category  Reading Comments  \\\n",
      "0  2022-06-08    分类：段子   阅读(57)    评论(0)   \n",
      "1  2022-06-01    分类：段子  阅读(242)    评论(0)   \n",
      "2  2022-05-26    分类：段子  阅读(249)    评论(0)   \n",
      "3  2022-05-26    分类：段子  阅读(278)    评论(0)   \n",
      "4  2022-05-25    分类：段子  阅读(202)    评论(0)   \n",
      "\n",
      "                                             Content  \n",
      "0  \\n小明考上了飞行员，但最近挺不开心的。虽然刚刚进入飞行训练阶段，但每次试飞教练都不让他参加...  \n",
      "1     \\n我：表哥，如果你女神突然像你表白，你会有什么反应？表哥：孩子生下来吧，算我的！我：……   \n",
      "2  \\n生活的现状是：想过八戒般的生活，但只有沙僧的本事，却承受着悟空般的压力，还时不时听到唐僧...  \n",
      "3  \\n我有一个女性朋友，长得蛮漂亮，身材也好，还有点小闷骚。。。。前段时间有两个男生同时追她，...  \n",
      "4  \\n在公共卫生间上完厕所，照了照镜子，忍不住赞叹自己：tmd，我长得真帅......然后听见...  \n"
     ]
    }
   ],
   "source": [
    "print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144b6fc",
   "metadata": {},
   "source": [
    "## Part 4 Export Dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4fa20281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Apple\\Desktop\\,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath(\",\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4673582",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"Jokes.csv\",encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceeb76f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24adcff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
